# -*- coding: utf-8 -*-
"""training_v0516_0900.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r1Dlea6Z_fF4gNJw7u48UsIDCeKv7VFS

## Datasets

- [Kkiller BirdClef 2021](https://www.kaggle.com/kneroma/kkiller-birdclef-2021)
- [Kkiller BirdCLEF Mels Computer D7 Part1](https://www.kaggle.com/kneroma/kkiller-birdclef-mels-computer-d7-part1)
- [Kkiller BirdCLEF Mels Computer D7 Part2](https://www.kaggle.com/kneroma/kkiller-birdclef-mels-computer-d7-part2)
- [Kkiller BirdCLEF Mels Computer D7 Part3](https://www.kaggle.com/kneroma/kkiller-birdclef-mels-computer-d7-part3)
- [Kkiller BirdCLEF Mels Computer D7 Part4](https://www.kaggle.com/kneroma/kkiller-birdclef-mels-computer-d7-part4)

## References

- https://www.kaggle.com/kneroma/birdclef-mels-computer-public/notebook
    - メルスペクトログラム(音声データを視覚化したもの 上記のデータセットに対応)の作り方
- https://www.kaggle.com/kneroma/clean-fast-simple-bird-identifier-training-colab
    - base
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# import os
# import sys
# 
# if "google.colab" in sys.modules:
#     from google.colab import drive
#     drive.mount('/content/drive')
#     %cd /content/drive/MyDrive/kaggle/kaggle-birdclef-2021/notebook/
#     !pip install timm
#     if not os.path.exists("/content/kkiller-birdclef-mels-computer-d7-part1"):
#         for index in range(1, 5):
#             !unzip ../download/kkiller-birdclef-mels-computer-d7-part{index}.zip -d /content/kkiller-birdclef-mels-computer-d7-part{index}/
# 
# !pip install -q pysndfx SoundFile audiomentations pretrainedmodels efficientnet_pytorch resnest
# 
# import re
# import numpy as np
# import pandas as pd
# import cv2
# 
# # pytorch
# import timm
# import torch
# from torch import nn
# from  torch.utils.data import Dataset, DataLoader
# from resnest.torch import resnest50
# import torch
# from torch import nn, optim
# from  torch.utils.data import Dataset, DataLoader
# from efficientnet_pytorch import EfficientNet
# import pretrainedmodels
# import resnest.torch as resnest_torch
# 
# # scikit-learn
# from sklearn.metrics import label_ranking_average_precision_score
# 
# # audio
# import soundfile as sf
# from  soundfile import SoundFile
# import librosa as lb
# import librosa.display as lbd
# 
# # utils
# import time
# import random
# import gc
# import re
# import json
# from ast import literal_eval
# import joblib
# from pathlib import Path
# from tqdm.notebook import tqdm
# 
# # visualization
# from matplotlib import pyplot as plt
# from IPython.display import Audio

sys.path.append("../lib/")
import birdclef

class Config:
    def __init__(self, debug:bool):
        self.debug = debug
        self.num_classes:int = 397
        self.sampling_rate:int = 32000
        self.max_read_samples:int = 5
        self.epochs:int = 30
        self.model_names = [
            # "resnext101_32x8d_wsl",
            # "resnext101_32x16d_wsl",
            # "resnext101_32x32d_wsl",
            "efficientnet-b0",
            # "efficientnet-b3",
            # "efficientnet-b5",
            # "densenet121",
        ] 

config = Config(debug=True)

NUM_CLASSES = 397
SR = 32_000
DURATION = 7
MAX_READ_SAMPLES = 5 # Each record will have 10 melspecs at most, you can increase this on Colab with High Memory Enabled

if "google.colab" in sys.modules:
    DATA_ROOT = "/content" 
    MEL_PATHS = sorted(Path("/content").glob("kkiller-birdclef-mels-computer-d7-part?/rich_train_metadata.csv"))
    TRAIN_LABEL_PATHS = sorted(Path("/content").glob("kkiller-birdclef-mels-computer-d7-part?/LABEL_IDS.json"))
else:  
    DATA_ROOT = Path("../input/birdclef-2021")
    MEL_PATHS = sorted(Path("../input").glob("kkiller-birdclef-mels-computer-d7-part?/rich_train_metadata.csv"))
    TRAIN_LABEL_PATHS = sorted(Path("../input").glob("kkiller-birdclef-mels-computer-d7-part?/LABEL_IDS.json"))

MODEL_ROOT = Path(".")

TRAIN_BATCH_SIZE = 32
TRAIN_NUM_WORKERS = 2

VAL_BATCH_SIZE = 32
VAL_NUM_WORKERS = 2

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print("Device:", DEVICE)

def get_df(mel_paths=MEL_PATHS, train_label_paths=TRAIN_LABEL_PATHS):
  df = None
  LABEL_IDS = {}
    
  for file_path in mel_paths:
    temp = pd.read_csv(str(file_path), index_col=0)
    temp["impath"] = temp.apply(lambda row: file_path.parent/"audio_images/{}/{}.npy".format(row.primary_label, row.filename), axis=1) 
    df = temp if df is None else df.append(temp)
    
  df["secondary_labels"] = df["secondary_labels"].apply(literal_eval)

  for file_path in train_label_paths:
    with open(str(file_path)) as f:
      LABEL_IDS.update(json.load(f))

  return LABEL_IDS, df

LABEL_IDS, df = get_df()

if config.debug:
    df = df.head(100)

print(df.shape)
df.head()

df["primary_label"].value_counts()

df["label_id"].min(), df["label_id"].max()



def load_data(df):
    def load_row(row):
        # impath = TRAIN_IMAGES_ROOT/f"{row.primary_label}/{row.filename}.npy"
        return row.filename, np.load(str(row.impath))[:MAX_READ_SAMPLES]
    pool = joblib.Parallel(4)
    mapper = joblib.delayed(load_row)
    tasks = [mapper(row) for row in df.itertuples(False)]
    res = pool(tqdm(tasks))
    res = dict(res)
    return res

audio_image_store = load_data(df)
len(audio_image_store)

print("shape:", next(iter(audio_image_store.values())).shape)
lbd.specshow(next(iter(audio_image_store.values()))[0])

pd.Series([len(x) for x in audio_image_store.values()]).value_counts()

class BirdClefDataset(Dataset):

    def __init__(self, audio_image_store, meta, sr=SR, is_train=True, num_classes=NUM_CLASSES, duration=DURATION):
        
        self.audio_image_store = audio_image_store
        self.meta = meta.copy().reset_index(drop=True)
        self.sr = sr
        self.is_train = is_train
        self.num_classes = num_classes
        self.duration = duration
        self.audio_length = self.duration*self.sr
    
    @staticmethod
    def normalize(image):
        image = image.astype("float32", copy=False) / 255.0
        image = np.stack([image, image, image])
        return image

    def __len__(self):
        return len(self.meta)
    
    def __getitem__(self, idx):
        row = self.meta.iloc[idx]
        image = self.audio_image_store[row.filename]

        image = image[np.random.choice(len(image))]
        image = self.normalize(image)
        
        
        t = np.zeros(self.num_classes, dtype=np.float32) + 0.0025 # Label smoothing
        t[row.label_id] = 0.995
        
        return image, t

ds = BirdClefDataset(audio_image_store, meta=df, sr=SR, duration=DURATION, is_train=True)
len(df)

x, y = ds[np.random.choice(len(ds))]
# x, y = ds[0]
x.shape, y.shape, np.where(y >= 0.5)

lbd.specshow(x[0])

def one_step( xb, yb, net, criterion, optimizer, scheduler=None):
  xb, yb = xb.to(DEVICE), yb.to(DEVICE)

  optimizer.zero_grad()
  o = net(xb)
  loss = criterion(o, yb)
  loss.backward()
  optimizer.step()
  
  with torch.no_grad():
      l = loss.item()

      o = o.sigmoid()
      yb = (yb > 0.5 )*1.0
      lrap = label_ranking_average_precision_score(yb.cpu().numpy(), o.cpu().numpy())

      o = (o > 0.5)*1.0

      prec = (o*yb).sum()/(1e-6 + o.sum())
      rec = (o*yb).sum()/(1e-6 + yb.sum())
      f1 = 2*prec*rec/(1e-6+prec+rec)

  if  scheduler is not None:
    scheduler.step()

  return l, lrap, f1.item(), rec.item(), prec.item()

@torch.no_grad()
def evaluate(net, criterion, val_laoder):
    net.eval()

    os, y = [], []
    val_laoder = tqdm(val_laoder, leave = False, total=len(val_laoder))

    for icount, (xb, yb) in  enumerate(val_laoder):

        y.append(yb.to(DEVICE))

        xb = xb.to(DEVICE)
        o = net(xb)

        os.append(o)

    y = torch.cat(y)
    o = torch.cat(os)

    l = criterion(o, y).item()
    
    o = o.sigmoid()
    y = (y > 0.5)*1.0

    lrap = label_ranking_average_precision_score(y.cpu().numpy(), o.cpu().numpy())

    o = (o > 0.5)*1.0

    prec = ((o*y).sum()/(1e-6 + o.sum())).item()
    rec = ((o*y).sum()/(1e-6 + y.sum())).item()
    f1 = 2*prec*rec/(1e-6+prec+rec)

    return l, lrap, f1, rec, prec,

def one_epoch(net, criterion, optimizer, scheduler, train_laoder, val_laoder):
  net.train()
  l, lrap, prec, rec, f1, icount = 0.,0.,0.,0., 0., 0
  train_laoder = tqdm(train_laoder, leave = False)
  epoch_bar = train_laoder
  
  for (xb, yb) in  epoch_bar:
      # epoch_bar.set_description("----|----|----|----|---->")
      _l, _lrap, _f1, _rec, _prec = one_step(xb, yb, net, criterion, optimizer)
      l += _l
      lrap += _lrap
      f1 += _f1
      rec += _rec
      prec += _prec

      icount += 1
        
      if hasattr(epoch_bar, "set_postfix") and not icount%10:
          epoch_bar.set_postfix(
            loss="{:.6f}".format(l/icount),
            lrap="{:.3f}".format(lrap/icount),
            prec="{:.3f}".format(prec/icount),
            rec="{:.3f}".format(rec/icount),
            f1="{:.3f}".format(f1/icount),
          )
  
  scheduler.step()

  l /= icount
  lrap /= icount
  f1 /= icount
  rec /= icount
  prec /= icount
  
  l_val, lrap_val, f1_val, rec_val, prec_val = evaluate(net, criterion, val_laoder)
  
  return (l, l_val), (lrap, lrap_val), (f1, f1_val), (rec, rec_val), (prec, prec_val)

def one_fold(model_name, fold, train_set, val_set, epochs=20, save=True, save_root=None):

    save_root = Path(save_root) or MODEL_ROOT

    saver = birdclef.callbacks.AutoSave(root=save_root, name=f"birdclef_{model_name}_fold{fold}", metric="f1_val")

    net = birdclef.models.get_model(model_name).to(DEVICE)

    criterion = nn.BCEWithLogitsLoss()

    optimizer = optim.Adam(net.parameters(), lr=8e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=1e-5, T_max=epochs)

    train_data = BirdClefDataset(
        audio_image_store,
        meta=df.iloc[train_set].reset_index(drop=True),
        sr=SR,
        duration=DURATION,
        is_train=True
    )
    train_laoder = DataLoader(
        train_data,
        batch_size=TRAIN_BATCH_SIZE,
        num_workers=TRAIN_NUM_WORKERS,
        shuffle=True,
        pin_memory=True
    )

    val_data = BirdClefDataset(
        audio_image_store,
        meta=df.iloc[val_set].reset_index(drop=True),
        sr=SR,
        duration=DURATION,
        is_train=False
    )
    val_laoder = DataLoader(
        val_data,
        batch_size=VAL_BATCH_SIZE,
        num_workers=VAL_NUM_WORKERS,
        shuffle=False
    )
    epochs_bar = tqdm(list(range(epochs)), leave=False)
    for epoch  in epochs_bar:
        epochs_bar.set_description(f"--> [EPOCH {epoch:02d}]")
        net.train()

        (l, l_val), (lrap, lrap_val), (f1, f1_val), (rec, rec_val), (prec, prec_val) = one_epoch(
            net=net,
            criterion=criterion,
            optimizer=optimizer,
            scheduler=scheduler,
            train_laoder=train_laoder,
            val_laoder=val_laoder,
        )

        epochs_bar.set_postfix(
            loss="({:.6f}, {:.6f})".format(l, l_val),
            prec="({:.3f}, {:.3f})".format(prec, prec_val),
            rec="({:.3f}, {:.3f})".format(rec, rec_val),
            f1="({:.3f}, {:.3f})".format(f1, f1_val),
            lrap="({:.3f}, {:.3f})".format(lrap, lrap_val),
        )

        print(
            "[{epoch:02d}] loss: {loss} lrap: {lrap} f1: {f1} rec: {rec} prec: {prec}".format(
                epoch=epoch,
                loss="({:.6f}, {:.6f})".format(l, l_val),
                prec="({:.3f}, {:.3f})".format(prec, prec_val),
                rec="({:.3f}, {:.3f})".format(rec, rec_val),
                f1="({:.3f}, {:.3f})".format(f1, f1_val),
                lrap="({:.3f}, {:.3f})".format(lrap, lrap_val),
            )
        )

        if save:
            metrics = {
                "loss": l, "lrap": lrap, "f1": f1, "rec": rec, "prec": prec,
                "loss_val": l_val, "lrap_val": lrap_val, "f1_val": f1_val, "rec_val": rec_val, "prec_val": prec_val,
                "epoch": epoch,
            }

            saver.log(net, metrics)

def train(model_name, epochs=20, save=True, n_splits=5, seed=177, save_root=None, suffix="", folds=None):
    gc.collect()
    torch.cuda.empty_cache()

    save_root = save_root or MODEL_ROOT/f"{model_name}{suffix}"
    save_root.mkdir(exist_ok=True, parents=True)

    fold_bar = tqdm(df.reset_index().groupby("fold").index.apply(list).items(), total=df.fold.max()+1)

    for fold, val_set in fold_bar:
        if folds and not fold in folds:
            continue

        print(f"\n############################### [FOLD {fold}]")
        fold_bar.set_description(f"[FOLD {fold}]")
        train_set = np.setdiff1d(df.index, val_set)

        one_fold(
            model_name,
            fold=fold,
            train_set=train_set,
            val_set=val_set,
            epochs=epochs,
            save=save,
            save_root=save_root
        )

        gc.collect()
        torch.cuda.empty_cache()

for model_name in config.model_names:
    print("\n\n###########################################", model_name.upper())
    train(
        model_name,
        epochs=config.epochs,
        suffix=f"_sr{SR}_d{DURATION}_v1_v1",
        folds=[0]
    )

